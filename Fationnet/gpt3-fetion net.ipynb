{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a3d39d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Administrator\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Administrator\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=LRASPP_MobileNet_V3_Large_Weights.COCO_WITH_VOC_LABELS_V1`. You can also use `weights=LRASPP_MobileNet_V3_Large_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "c:\\Users\\Administrator\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DeepLabV3_ResNet50_Weights.COCO_WITH_VOC_LABELS_V1`. You can also use `weights=DeepLabV3_ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 2 persons, 3 cars, 135.2ms\n",
      "Speed: 10.9ms preprocess, 135.2ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 5 persons, 1 car, 132.6ms\n",
      "Speed: 2.4ms preprocess, 132.6ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (41,19) (520,520) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 182\u001b[0m\n\u001b[0;32m    179\u001b[0m             plot_histogram(crop, hands, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPerson \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Hand HSV Histogram\u001b[39m\u001b[38;5;124m\"\u001b[39m, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(hist_dir, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbasename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_p\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_hand_hist.png\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m    181\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 182\u001b[0m     process_all()\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing complete. Segmented images saved to:\u001b[39m\u001b[38;5;124m\"\u001b[39m, OUTPUT_DIR)\n",
      "Cell \u001b[1;32mIn[1], line 156\u001b[0m, in \u001b[0;36mprocess_all\u001b[1;34m()\u001b[0m\n\u001b[0;32m    154\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m    155\u001b[0m upper, lower, hands, coords \u001b[38;5;241m=\u001b[39m split_parts(mask, landmarks)\n\u001b[1;32m--> 156\u001b[0m upper_feat \u001b[38;5;241m=\u001b[39m extract_clothing_features(crop, upper)\n\u001b[0;32m    157\u001b[0m lower_feat \u001b[38;5;241m=\u001b[39m extract_clothing_features(crop, lower)\n\u001b[0;32m    158\u001b[0m person_id_vector \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate([upper_feat, lower_feat])\n",
      "Cell \u001b[1;32mIn[1], line 118\u001b[0m, in \u001b[0;36mextract_clothing_features\u001b[1;34m(region_img, region_mask)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_clothing_features\u001b[39m(region_img, region_mask):\n\u001b[0;32m    116\u001b[0m     \u001b[38;5;66;03m# Apply clothing-only mask\u001b[39;00m\n\u001b[0;32m    117\u001b[0m     clothing_mask \u001b[38;5;241m=\u001b[39m get_clothing_mask(region_img)\n\u001b[1;32m--> 118\u001b[0m     refined_mask \u001b[38;5;241m=\u001b[39m region_mask \u001b[38;5;241m&\u001b[39m clothing_mask\n\u001b[0;32m    120\u001b[0m     region \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mbitwise_and(region_img, region_img, mask\u001b[38;5;241m=\u001b[39mrefined_mask\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39muint8))\n\u001b[0;32m    121\u001b[0m     hsv \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(region, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2HSV)\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (41,19) (520,520) "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torchvision.models.segmentation import deeplabv3_resnet50\n",
    "from ultralytics import YOLO\n",
    "import mediapipe as mp\n",
    "from skimage.feature import local_binary_pattern, graycomatrix, graycoprops\n",
    "from skimage.color import rgb2gray\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Paths - adjust to your local setup\n",
    "# Paths - adjust to your local setup\n",
    "INPUT_DIR = 'Time_12-34'              # folder containing JPEG frames\n",
    "OUTPUT_DIR = 'output_segment' \n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Device for PyTorch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load YOLOv8n for person detection\n",
    "yolo_model = YOLO('yolov8n.pt')\n",
    "\n",
    "# Load Clothing Segmentation Model (e.g., substitute for FashionNet)\n",
    "from torchvision.models.segmentation import lraspp_mobilenet_v3_large\n",
    "\n",
    "def load_clothing_segmentation_model():\n",
    "    model = lraspp_mobilenet_v3_large(pretrained=True)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "clothing_model = load_clothing_segmentation_model()\n",
    "\n",
    "# Predict clothing mask using segmentation output (e.g., label 5 = shirt, 6 = pants)\n",
    "def get_clothing_mask(image):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = seg_transform(image).unsqueeze(0).to(device)\n",
    "        output = clothing_model(input_tensor)['out']\n",
    "        pred = torch.argmax(output.squeeze(), dim=0).cpu().numpy()\n",
    "        clothing_mask = (pred == 5) | (pred == 6) | (pred == 7)  # shirt, pants, dress\n",
    "        return clothing_mask.astype(bool)\n",
    "\n",
    "# Placeholder for now - replace or integrate real model in place of DeepLabV3\n",
    "def load_segmentation_model():\n",
    "    model = deeplabv3_resnet50(pretrained=True, progress=True)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "segmentation_model = load_segmentation_model()\n",
    "\n",
    "# Preprocessing transform for segmentation\n",
    "seg_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((520, 520)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Initialize MediaPipe Pose\n",
    "mp_pose = mp.solutions.pose.Pose(static_image_mode=True)\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Segmentation mask from current model\n",
    "def get_person_mask_resnet(crop):\n",
    "    img = cv2.cvtColor(crop, cv2.COLOR_BGR2RGB)\n",
    "    inp = seg_transform(img).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        out = segmentation_model(inp)['out'][0]\n",
    "        mask = torch.sigmoid(out[15]).cpu().numpy() > 0.5\n",
    "    return cv2.resize(mask.astype(np.uint8), (crop.shape[1], crop.shape[0]), interpolation=cv2.INTER_NEAREST).astype(bool)\n",
    "\n",
    "# Extract pose landmarks\n",
    "def get_landmarks(crop):\n",
    "    rgb = cv2.cvtColor(crop, cv2.COLOR_BGR2RGB)\n",
    "    return mp_pose.process(rgb).pose_landmarks\n",
    "\n",
    "# Split body parts\n",
    "def split_parts(mask, landmarks):\n",
    "    H, W = mask.shape\n",
    "    coords = {mp.solutions.pose.PoseLandmark(idx).name: (int(lm.x * W), int(lm.y * H)) for idx, lm in enumerate(landmarks.landmark)}\n",
    "    hip_y = int((coords['LEFT_HIP'][1] + coords['RIGHT_HIP'][1]) / 2)\n",
    "    rows = np.arange(H)[:, None]\n",
    "    upper = mask & (rows < hip_y)\n",
    "    lower = mask & (rows >= hip_y)\n",
    "    hands = np.zeros_like(mask, dtype=bool)\n",
    "    for side in ['LEFT', 'RIGHT']:\n",
    "        key = f'{side}_WRIST'\n",
    "        if key in coords:\n",
    "            wx, wy = coords[key]\n",
    "            r = int(0.1 * H)\n",
    "            hands[max(0, wy-r):min(H, wy+r), max(0, wx-r):min(W, wx+r)] = True\n",
    "    return upper, lower, hands, coords\n",
    "\n",
    "# Plot and save HSV histograms\n",
    "def plot_histogram(region_img, mask, title, save_path=None):\n",
    "    hsv = cv2.cvtColor(region_img, cv2.COLOR_BGR2HSV)\n",
    "    hist = cv2.calcHist([hsv], [0], mask.astype(np.uint8), [180], [0, 180])\n",
    "    plt.figure(figsize=(4, 2))\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Hue\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.plot(hist, color='orange')\n",
    "    plt.xlim([0, 180])\n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "    plt.close()\n",
    "\n",
    "# Feature extraction: refine region_mask using FashionNet-style clothing mask before histograms\n",
    "\n",
    "def extract_clothing_features(region_img, region_mask):\n",
    "    # Apply clothing-only mask\n",
    "    clothing_mask = get_clothing_mask(region_img)\n",
    "    clothing_mask = cv2.resize(clothing_mask.astype(np.uint8), (region_img.shape[1], region_img.shape[0]), interpolation=cv2.INTER_NEAREST).astype(bool)\n",
    "    refined_mask = region_mask & clothing_mask\n",
    "\n",
    "    region = cv2.bitwise_and(region_img, region_img, mask=refined_mask.astype(np.uint8))\n",
    "    hsv = cv2.cvtColor(region, cv2.COLOR_BGR2HSV)\n",
    "    hist_h = cv2.calcHist([hsv], [0], refined_mask.astype(np.uint8), [16], [0, 180])\n",
    "    hist_s = cv2.calcHist([hsv], [1], refined_mask.astype(np.uint8), [16], [0, 256])\n",
    "    hist_v = cv2.calcHist([hsv], [2], refined_mask.astype(np.uint8), [16], [0, 256])\n",
    "    color_hist = np.concatenate([hist_h, hist_s, hist_v]).flatten()\n",
    "    color_hist /= (np.sum(color_hist) + 1e-6)\n",
    "    gray = rgb2gray(region)\n",
    "    lbp = local_binary_pattern(gray, P=8, R=1, method='uniform')\n",
    "    lbp_hist, _ = np.histogram(lbp[refined_mask], bins=10, range=(0, 10), density=True)\n",
    "    return np.concatenate([color_hist, lbp_hist])\n",
    "\n",
    "# GLCM texture features\n",
    "def get_texture_features(gray_img):\n",
    "    glcm = graycomatrix((gray_img * 255).astype(np.uint8), distances=[1], angles=[0], levels=256, symmetric=True, normed=True)\n",
    "    return tuple(graycoprops(glcm, prop)[0, 0] for prop in ['contrast', 'energy', 'homogeneity'])\n",
    "\n",
    "# Main processing\n",
    "def process_all():\n",
    "    imgs = glob.glob(os.path.join(INPUT_DIR, '**', '*.jpg'), recursive=True)\n",
    "    for img_path in imgs:\n",
    "        img = cv2.imread(img_path)\n",
    "        if img is None:\n",
    "            continue\n",
    "        results = yolo_model(img)[0]\n",
    "        person_boxes = [box.xyxy[0].cpu().numpy().astype(int) for box, cls in zip(results.boxes, results.boxes.cls) if int(cls) == 0]\n",
    "        rel_dir = os.path.relpath(os.path.dirname(img_path), INPUT_DIR)\n",
    "        out_dir = os.path.join(OUTPUT_DIR, rel_dir)\n",
    "        os.makedirs(out_dir, exist_ok=True)\n",
    "        for i, (x1, y1, x2, y2) in enumerate(person_boxes):\n",
    "            crop = img[y1:y2, x1:x2]\n",
    "            mask = get_person_mask_resnet(crop)\n",
    "            landmarks = get_landmarks(crop)\n",
    "            if not landmarks:\n",
    "                continue\n",
    "            upper, lower, hands, coords = split_parts(mask, landmarks)\n",
    "            upper_feat = extract_clothing_features(crop, upper)\n",
    "            lower_feat = extract_clothing_features(crop, lower)\n",
    "            person_id_vector = np.concatenate([upper_feat, lower_feat])\n",
    "            print(f\"Person {i} features (Upper+Lower):\", person_id_vector[:10])\n",
    "            gray_crop = cv2.cvtColor(crop, cv2.COLOR_BGR2GRAY)\n",
    "            contrast, energy, homogeneity = get_texture_features(gray_crop)\n",
    "            print(f\"Person {i+1} Texture - Contrast: {contrast}, Energy: {energy}, Homogeneity: {homogeneity})\")\n",
    "            vis = crop.copy()\n",
    "            if 'LEFT_SHOULDER' in coords and 'RIGHT_SHOULDER' in coords:\n",
    "                cv2.line(vis, coords['LEFT_SHOULDER'], coords['RIGHT_SHOULDER'], (0, 0, 255), 2)\n",
    "            if 'LEFT_HIP' in coords and 'RIGHT_HIP' in coords:\n",
    "                cv2.line(vis, coords['LEFT_HIP'], coords['RIGHT_HIP'], (0, 255, 0), 2)\n",
    "            for side in ['LEFT', 'RIGHT']:\n",
    "                key = f'{side}_WRIST'\n",
    "                if key in coords:\n",
    "                    cv2.circle(vis, coords[key], 5, (255, 0, 0), -1)\n",
    "            basename = os.path.splitext(os.path.basename(img_path))[0]\n",
    "            out_path = os.path.join(out_dir, f\"{basename}_p{i}.jpg\")\n",
    "            cv2.imwrite(out_path, vis)\n",
    "            hist_dir = os.path.join(out_dir, \"histograms\")\n",
    "            os.makedirs(hist_dir, exist_ok=True)\n",
    "            plot_histogram(crop, upper, f\"Person {i+1} Upper HSV Histogram\", os.path.join(hist_dir, f\"{basename}_p{i}_upper_hist.png\"))\n",
    "            plot_histogram(crop, lower, f\"Person {i+1} Lower HSV Histogram\", os.path.join(hist_dir, f\"{basename}_p{i}_lower_hist.png\"))\n",
    "            plot_histogram(crop, hands, f\"Person {i+1} Hand HSV Histogram\", os.path.join(hist_dir, f\"{basename}_p{i}_hand_hist.png\"))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    process_all()\n",
    "    print(\"Processing complete. Segmented images saved to:\", OUTPUT_DIR)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
